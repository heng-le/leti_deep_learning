{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb51ce3-bc48-45b5-af19-d4071976991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b338ea-a167-4e4a-a174-0b09a378d688",
   "metadata": {},
   "source": [
    "### Load RNA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26424f2e-cd61-4166-9999-9cae2b571a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPN15k 15000\n",
      "PK50 2729\n",
      "PK90 2173\n",
      "R1 119999\n"
     ]
    }
   ],
   "source": [
    "dict_dataset = {\n",
    "    'GPN15k': pd.read_csv('data/GPN15k_silico_predictions.csv'),\n",
    "    'PK50': pd.read_csv('data/PK50_silico_predictions.csv'),\n",
    "    'PK90': pd.read_csv('data/PK90_silico_predictions.csv'),\n",
    "    'R1': pd.read_csv('data/R1_silico_predictions.csv'),\n",
    "}\n",
    "\n",
    "RNA_sequences = []\n",
    "RNA_structures = []\n",
    "\n",
    "for k, df_data in dict_dataset.items():\n",
    "    df_data = df_data[~df_data.vienna2_mfe.str.contains('x')]\n",
    "    RNA_sequences = RNA_sequences + df_data.sequence.to_list()\n",
    "    RNA_structures = RNA_structures + df_data.vienna2_mfe.to_list()\n",
    "    print(k, len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c43dce-e790-474b-a3d6-87c4fe6da307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107723 32178\n",
      "107723 32178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RNA_seq_train_val, RNA_seq_test, RNA_struct_train_val, RNA_struct_test = train_test_split(\n",
    "    RNA_sequences, RNA_structures, test_size=0.23, random_state=42, shuffle=True)\n",
    "\n",
    "RNA_seq_train, RNA_seq_val, RNA_struct_train, RNA_struct_val = train_test_split(\n",
    "    RNA_seq_train_val, RNA_struct_train_val, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b0d57-646f-4980-9c9f-07615cbc7984",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a83128e-be90-4812-9e22-a3a2bd6839f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(sequence, structure):\n",
    "    base_to_idx = {'A': 0, 'U': 1, 'G': 2, 'C': 3}\n",
    "    struct_to_idx = {'.': 0, '(': 1, ')': 2}\n",
    "    \n",
    "    node_features = []\n",
    "    for base, struct in zip(sequence, structure):\n",
    "        base_feature = [0, 0, 0, 0]\n",
    "        base_feature[base_to_idx[base]] = 1\n",
    "        struct_feature = [0, 0, 0]\n",
    "        struct_feature[struct_to_idx[struct]] = 1\n",
    "        node_features.append(base_feature + struct_feature)\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    edges = []\n",
    "    stack = []\n",
    "    for i, (base, struct) in enumerate(zip(sequence, structure)):\n",
    "        if i > 0:\n",
    "            edges.append((i-1, i))\n",
    "            edges.append((i, i-1))\n",
    "        \n",
    "        if struct == '(':\n",
    "            stack.append(i)\n",
    "        elif struct == ')' and stack:\n",
    "            j = stack.pop()\n",
    "            edges.append((i, j))\n",
    "            edges.append((j, i))\n",
    "    for i in range(len(sequence) - 1):\n",
    "        j = i + 1\n",
    "        edges.append((i, j))\n",
    "        edges.append((j, i))\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# Convert data to graphs\n",
    "graphs_train = [build_graph(seq, struct) for seq, struct in zip(RNA_seq_train, RNA_struct_train)]\n",
    "graphs_test = [build_graph(seq, struct) for seq, struct in zip(RNA_seq_test, RNA_struct_test)]\n",
    "graphs_val = [build_graph(seq, struct) for seq, struct in zip(RNA_seq_val, RNA_struct_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0746ee6-797b-4df7-be4d-f3de44f99868",
   "metadata": {},
   "source": [
    "### GNN-AutoEncoder Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b389f079-c61c-4d61-86b6-f0caa91c068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAGraphAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(RNAGraphAutoencoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList([\n",
    "            GCNConv(input_dim, hidden_dim),\n",
    "            GCNConv(hidden_dim, hidden_dim),\n",
    "            GCNConv(hidden_dim, latent_dim)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.ModuleList([\n",
    "            GCNConv(latent_dim, hidden_dim),\n",
    "            GCNConv(hidden_dim, hidden_dim),\n",
    "            GCNConv(hidden_dim, input_dim)\n",
    "        ])\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        for layer in self.encoder:\n",
    "            x = self.relu(layer(x, edge_index))\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, edge_index):\n",
    "        for layer in self.decoder:\n",
    "            x = self.relu(layer(x, edge_index))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Encoding\n",
    "        latent = self.encode(x, edge_index)\n",
    "        \n",
    "        # Decoding\n",
    "        reconstructed = self.decode(latent, edge_index)\n",
    "        \n",
    "        return reconstructed, latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d4cbd2-308d-4663-a044-d9f7191c4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 7  # 4 for bases + 3 for structure\n",
    "hidden_dim = 64\n",
    "latent_dim = 32\n",
    "model = RNAGraphAutoencoder(input_dim, hidden_dim, latent_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bad518-9f9b-46da-bc05-dfb5b0cee18e",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03136b4c-15e2-403a-b399-07fcf592bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/betorch/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(graphs_train, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(graphs_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f0d3b2-9b8e-4e16-ace2-a9dae0930d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.1301\n",
      "Epoch 2/100, Loss: 0.1143\n",
      "Epoch 3/100, Loss: 0.1078\n",
      "Epoch 4/100, Loss: 0.1044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 24\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN-AutoEncoder training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed, batch\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/betorch/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/betorch/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, _ = model(batch)\n",
    "        loss = criterion(reconstructed, batch.x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            reconstructed, _ = model(batch)\n",
    "            loss = criterion(reconstructed, batch.x)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, data_loader, criterion, optimizer)\n",
    "    val_loss = validate(model, val_data_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"GNN-AutoEncoder training completed!\")\n",
    "\n",
    "torch.save(train_losses, 'train_losses.pth')\n",
    "torch.save(val_losses, 'val_losses.pth')\n",
    "\n",
    "\n",
    "plt.rc('font', size=14)  \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0968e7-3f71-47e9-a1a4-759cd35d3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"autoencoder_model_v2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9efc0-2048-41b7-ac9c-b8ca5bd0edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(model, loader):\n",
    "#     model.eval()\n",
    "#     features = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             _, latent = model(batch)\n",
    "#             features.append(latent)\n",
    "    \n",
    "#     return torch.cat(features, dim=0)\n",
    "\n",
    "# # Extract features for all graphs\n",
    "# feature_loader = DataLoader(graphs_test, batch_size=batch_size, shuffle=False)\n",
    "# extracted_features = extract_features(model, feature_loader)\n",
    "\n",
    "# print(f\"Extracted features shape: {extracted_features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
